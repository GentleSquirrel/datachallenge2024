{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ouvre le xlsx Dataset \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#ouvre le xlsx Dataset\n",
    "df = pd.read_excel('Dataset.xlsx')\n",
    "# Supposons que votre DataFrame s'appelle df\n",
    "btc_columns = df.filter(like='BTC').columns.tolist() + df.filter(like='btc').columns.tolist()\n",
    "#sors moi un df avec les colonnes btc, en ajoutant la colonne Date\n",
    "df_btc = df[btc_columns + ['date']]\n",
    "#sur la colonne Close, fait un pct change dans une nouvelle colonne Return\n",
    "df_btc['Return_BTC'] = df_btc['Close_BTC'].pct_change()\n",
    "#drop la premiere ligne de df_btc\n",
    "df_btc = df_btc.drop(df_btc.index[0])\n",
    "# Supposons que 'value' est la colonne contenant les valeurs que vous voulez comparer\n",
    "df_btc['target'] = (df_btc['Return_BTC'].shift(-1) > df_btc['Return_BTC']).astype(int)\n",
    "# Supposons que 'date' est la colonne contenant les dates\n",
    "df_btc['date'] = pd.to_datetime(df_btc['date'])  # Assurez-vous que la colonne est de type datetime\n",
    "df_btc['year'] = df_btc['date'].dt.year\n",
    "df_btc['month'] = df_btc['date'].dt.month\n",
    "df_btc['day'] = df_btc['date'].dt.day\n",
    "df_btc['day_of_week'] = df_btc['date'].dt.dayofweek  # Lundi=0, Dimanche=6\n",
    "\n",
    "#fais un fillna 0\n",
    "df_btc = df_btc.fillna(0)\n",
    "#fais le train de 2017-08 a 2022-08 et le reste en test\n",
    "df_train = df_btc[df_btc['date'] < '2022-08-01']\n",
    "df_test = df_btc[df_btc['date'] >= '2022-08-01']\n",
    "##mets la colonne date en index \n",
    "df_train = df_train.set_index('date')\n",
    "df_test = df_test.set_index('date')\n",
    "\n",
    "X_train = df_train.drop('target',axis=1)\n",
    "y_train = df_train['target']\n",
    "X_test = df_test.drop('target',axis=1)\n",
    "y_test = df_test['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_btc_scaled = df_btc.copy()\n",
    "\n",
    "# Obtenir toutes les colonnes sauf la colonne de date\n",
    "columns_to_scale = df_btc.columns.drop(['date', 'target','day','month','year','day_of_week'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_btc_scaled[columns_to_scale] = scaler.fit_transform(df_btc[columns_to_scale])\n",
    "\n",
    "df_btc_scaled.head()\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df_btc_scaled_mm = df_btc.copy()\n",
    "\n",
    "mmscaler = MinMaxScaler()\n",
    "df_btc_scaled_mm[columns_to_scale] = mmscaler.fit_transform(df_btc[columns_to_scale])\n",
    "\n",
    "df_btc_scaled_mm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def train_random_forest(X_train, y_train, X_test):\n",
    "    \n",
    "    # Créer le modèle de forêt aléatoire\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Prédire les valeurs de l'ensemble de test\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    # Afficher la précision du modèle\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "    #montre le RMSE \n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    print('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\n",
    "\n",
    "    #compte le nombre de 0 de df \n",
    "    print(X_test.value_counts())\n",
    "    # Retourner le modèle entraîné et les prédictions\n",
    "    return rf, y_pred\n",
    "\n",
    "# Utiliser la fonction\n",
    "rf_model, rf_pred = train_random_forest(X_train, y_train, X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Logistic Regression\n",
    "    logreg = LogisticRegression(random_state=42)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    y_pred = logreg.predict(X_test)\n",
    "\n",
    "    # Evaluation: Confusion matrix\n",
    "    logreg_acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred) # Confusion matrix\n",
    "    tpr_logreg = cm[1][1] /(cm[1][0] + cm[1][1])\n",
    "\n",
    "    print('The accuracy score is:', logreg_acc) # accuracy score\n",
    "    print('Sensitivity (TPR) =', tpr_logreg)\n",
    "\n",
    "    print('\\n Confusion matrix \\n \\n')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Calculate the RMSE \n",
    "    print('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "logpred = logistic_regression(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def linear_discriminant_analysis(X_train, y_train, X_test, y_test):\n",
    "    lda = LinearDiscriminantAnalysis(solver='lsqr', store_covariance=True)\n",
    "    lda.fit(X_train, y_train)\n",
    "\n",
    "    # Predict Test Set Responses\n",
    "    y_predicted = lda.predict(X_test)\n",
    "    y_predicted= np.array(y_predicted > 0.5, dtype=float)\n",
    "\n",
    "    # Evaluation: Confusion matrix\n",
    "    lda_acc = accuracy_score(y_test, y_predicted)  # accuracy score\n",
    "    cm_lda = confusion_matrix(y_test, y_predicted) # Confusion matrix \n",
    "    tpr_lda = cm_lda[1][1] /(cm_lda[1][0] + cm_lda[1][1])\n",
    "\n",
    "    print('Accuracy =', lda_acc)  \n",
    "    print('Sensitivity (TPR) =', tpr_lda)\n",
    "\n",
    "    print('\\n Confusion matrix \\n \\n')\n",
    "    print(classification_report(y_test, y_predicted ))\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    print('RMSE:', mean_squared_error(y_test, y_predicted, squared=False))\n",
    "    return y_predicted\n",
    "\n",
    "#lance la fonction\n",
    "discri_pred = linear_discriminant_analysis(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error\n",
    "\n",
    "def decision_tree_classifier(X_train, y_train, X_test, y_test):\n",
    "    dtree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "    # Build classification tree\n",
    "    dtree.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = dtree.predict(X_test)\n",
    "\n",
    "    # Evaluation: Confusion matrix\n",
    "    dtree_acc = accuracy_score(y_test, y_pred)   # accuracy score\n",
    "    cm_dtree = confusion_matrix(y_test, y_pred) # Confusion matrix \n",
    "    tpr_dtree = cm_dtree[1][1] /(cm_dtree[1][0] + cm_dtree[1][1])\n",
    "\n",
    "    print(\"Accuracy:\",dtree_acc)\n",
    "    print('Sensitivity (TPR) =', tpr_dtree)\n",
    "    print('\\n Confusion matrix \\n \\n')\n",
    "    print(classification_report(y_test, y_pred ))\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    print('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\n",
    "    return y_pred\n",
    "\n",
    "decision_pred = decision_tree_classifier(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error\n",
    "\n",
    "def gradient_boosting_classifier(X_train, y_train, X_test, y_test):\n",
    "    booster = GradientBoostingClassifier(max_depth=7,n_estimators=50,min_samples_split=1400,min_samples_leaf=60,max_features=7,subsample=0.85)\n",
    "    boost_est = booster.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = boost_est.predict(X_test)\n",
    "\n",
    "    # Evaluation: Confusion matrix\n",
    "    boosting_acc = accuracy_score(y_test, y_pred) #accuracy score\n",
    "    cm_bossting = confusion_matrix(y_test, y_pred) # Confusion matrix \n",
    "    tpr_boost = cm_bossting[1][1] /(cm_bossting[1][0] + cm_bossting[1][1]) #Sensitivity (TPR)\n",
    "\n",
    "    print('Accuracy:', boosting_acc) # accuracy score\n",
    "    print('Sensitivity (TPR) =', tpr_boost)\n",
    "\n",
    "    print('\\n Confusion matrix \\n \\n')\n",
    "    print(classification_report(y_test, y_pred ))\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    print('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\n",
    "    return y_pred\n",
    "\n",
    "gradient_pred = gradient_boosting_classifier(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input les données normaliser patati patata\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Creating classifiers for every value of K\n",
    "classifiers = []\n",
    "nb_neighbor = [1, 5, 10, 20, 30, 40, 50]\n",
    "for i in range(len(nb_neighbor)):\n",
    "    classifiers.append(KNeighborsClassifier(nb_neighbor[i]))\n",
    "\n",
    "# Initializing the lists for accuracy, true positive rate and true negative rate\n",
    "# Later used to compare the classifiers for different values of K\n",
    "score_list = []\n",
    "true_positive = []\n",
    "true_negative = []\n",
    "\n",
    "# Fitting the training dataset for every classifier and calculating metrics\n",
    "\n",
    "index = 0\n",
    "for clf in classifiers: \n",
    "    clf.fit(X_train,y_train)\n",
    "\n",
    "    score = clf.score(X_test, y_test)  \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(f\"Accuracy for K =\", nb_neighbor[index] ,\"nearest Neighbors: \",  accuracy_score(y_test, y_pred))\n",
    "    # print le rmse\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    print('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\n",
    "\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred) # Confusion matrix  \n",
    "\n",
    "    score_list.append(score)\n",
    "    true_positive.append(cm[1][1])\n",
    "    true_negative.append(cm[0][0])\n",
    "    \n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def quadratic_discriminant_analysis(X_train, y_train, X_test, y_test):\n",
    "    qdaClassifier = QuadraticDiscriminantAnalysis(store_covariance=True)\n",
    "    qdaClassifier.fit(X_train,y_train)\n",
    "\n",
    "    # Get predictions\n",
    "    y_predict = qdaClassifier.predict(X_test)\n",
    "    y_predicted= np.array(y_predict > 0.5, dtype=float)\n",
    "\n",
    "    # Get evaluation criteria\n",
    "    qda_acc = accuracy_score(y_test, y_predicted) \n",
    "    qda_cm = confusion_matrix(y_test, y_predicted)\n",
    "    qda_tpr = qda_cm[1][1] /(qda_cm[1][0] + qda_cm[1][1])\n",
    "\n",
    "    print('Accuracy =', qda_acc)\n",
    "    print('Sensitivity (TPR) =', qda_tpr)\n",
    "\n",
    "    print('\\n Confusion matrix \\n \\n')\n",
    "    print(classification_report(y_test, y_predicted ))\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    print('RMSE:', mean_squared_error(y_test, y_predicted, squared=False))\n",
    "    return y_predicted\n",
    "\n",
    "quadratic_pred = quadratic_discriminant_analysis(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "\n",
    "def xgboost_classifier(X_train, y_train, X_test, y_test, df_test):\n",
    "    dtrain_clf = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "    dtest_clf = xgb.DMatrix(X_test, y_test, enable_categorical=True)\n",
    "    dpred_clf = xgb.DMatrix(df_test, enable_categorical=True)\n",
    "\n",
    "    params = {\"objective\": \"multi:softmax\",\"num_class\": 3, \"tree_method\": \"hist\",\n",
    "                \"learning_rate\": 0.3, \"max_depth\": 6,\n",
    "                \"gamma\": 0, \"subsample\": 1, \"colsample_bytree\": 1,\n",
    "                \"alpha\": 0, \"lambda\": 1,\"random_state\": 0}\n",
    "\n",
    "    n = 50\n",
    "    evals = [(dtest_clf, \"validation\"), (dtrain_clf, \"train\")]\n",
    "\n",
    "    model = xgb.train(\n",
    "       params=params,\n",
    "       dtrain=dtrain_clf,\n",
    "       num_boost_round=n,\n",
    "       evals=evals,\n",
    "       verbose_eval=1,\n",
    "       # Activate early stopping\n",
    "       early_stopping_rounds=30\n",
    "    )\n",
    "    preds = model.predict(dtest_clf)\n",
    "    rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "    accuracy = accuracy_score(y_test, preds.round())\n",
    "\n",
    "    print(f\"RMSE of the base model: {rmse:.3f}\")\n",
    "    print(f\"Accuracy of the base model: {accuracy:.3f}\") \n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, preds)\n",
    "    print(\"Confusion matrix: \\n\", conf_matrix)\n",
    "    return preds\n",
    "\n",
    "xgb_pred = xgboost_classifier(X_train, y_train, X_test, y_test, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Supposons que preds_dtree, preds_boost, preds_qda et preds_xgb sont vos prédictions\n",
    "df_preds = pd.DataFrame({\n",
    "    'RF': rf_pred,\n",
    "    'logreg': logpred,\n",
    "    'discri': discri_pred,\n",
    "    'dtree': decision_pred,\n",
    "    'boost': gradient_pred,\n",
    "    'qda': quadratic_pred,\n",
    "    'xgb': xgb_pred\n",
    "}, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common Model Algorithms\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "import time\n",
    "\n",
    "#Configure Visualization Defaults\n",
    "#%matplotlib inline = show plots in Jupyter Notebook browser\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Machine Learning Algorithm (MLA) Selection and Initialization\n",
    "# MLA = [\n",
    "#         #Ensemble Methods\n",
    "#         ensemble.AdaBoostClassifier(),\n",
    "#         ensemble.BaggingClassifier(),\n",
    "#         ensemble.ExtraTreesClassifier(),\n",
    "#         ensemble.GradientBoostingClassifier(),\n",
    "#         ensemble.RandomForestClassifier(),\n",
    "\n",
    "#         #Gaussian Processes\n",
    "#         gaussian_process.GaussianProcessClassifier(),\n",
    "        \n",
    "#         #GLM\n",
    "#         linear_model.LogisticRegressionCV(),\n",
    "#         linear_model.PassiveAggressiveClassifier(),\n",
    "#         linear_model.RidgeClassifierCV(),\n",
    "#         linear_model.SGDClassifier(),\n",
    "#         linear_model.Perceptron(),\n",
    "        \n",
    "#         #Navies Bayes\n",
    "#         naive_bayes.BernoulliNB(),\n",
    "#         naive_bayes.GaussianNB(),\n",
    "        \n",
    "#         #Nearest Neighbor\n",
    "#         neighbors.KNeighborsClassifier(),\n",
    "        \n",
    "#         #SVM\n",
    "#         svm.SVC(probability=True),\n",
    "        \n",
    "#         #Trees    \n",
    "#         tree.DecisionTreeClassifier(),\n",
    "#         tree.ExtraTreeClassifier(),\n",
    "        \n",
    "#         #Discriminant Analysis\n",
    "#         discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "#         discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "\n",
    "    \n",
    "#         #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "#         XGBClassifier()    \n",
    "#         ]\n",
    "\n",
    "\n",
    "\n",
    "# #split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n",
    "# #note: this is an alternative to train_test_split\n",
    "# #cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .7, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n",
    "\n",
    "# #create table to compare MLA metrics\n",
    "# MLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy Mean','MLA Time','MLA RMSE']\n",
    "# MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "# #create table to compare MLA predictions\n",
    "# MLA_predict = y_train.copy()\n",
    "\n",
    "# #index through MLA and save performance to table\n",
    "# row_index = 0\n",
    "# for alg in MLA:\n",
    "\n",
    "#     #set name and parameters\n",
    "#     MLA_name = alg.__class__.__name__\n",
    "#     MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "#     MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    \n",
    "#     # #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "#     # cv_results = model_selection.cross_validate(alg, X_train, y_train, cv  = cv_split)\n",
    "\n",
    "#     # #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n",
    "#     # MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n",
    "    \n",
    "\n",
    "#     #save MLA predictions - see section 6 for usag\n",
    "#     start_time = time.time()\n",
    "#     alg.fit(X_train, y_train)\n",
    "#     predictions = alg.predict(X_test)\n",
    "#     end_time = time.time()\n",
    "#     # Calculer le RMSE et l'ajouter à la table\n",
    "#     accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "#     rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "#     # Save the metrics to the table\n",
    "#     MLA_compare.loc[row_index, 'MLA Time'] = end_time - start_time\n",
    "#     MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = accuracy\n",
    "#     MLA_compare.loc[row_index, 'MLA RMSE'] = rmse\n",
    "    \n",
    "    \n",
    "#     MLA_predict[MLA_name] = predictions\n",
    "    \n",
    "#     row_index+=1\n",
    "\n",
    "    \n",
    "# #print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\n",
    "# MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "# MLA_compare1 = MLA_compare.copy()\n",
    "# MLA_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ml_models(X_train, y_train, X_test, y_test,df):\n",
    "    from sklearn import ensemble, gaussian_process, linear_model, naive_bayes, neighbors, svm, tree, discriminant_analysis\n",
    "    from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "    from xgboost import XGBClassifier\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import time\n",
    "\n",
    "    MLA = [\n",
    "        ensemble.AdaBoostClassifier(),\n",
    "        ensemble.BaggingClassifier(),\n",
    "        ensemble.ExtraTreesClassifier(),\n",
    "        ensemble.GradientBoostingClassifier(),\n",
    "        ensemble.RandomForestClassifier(),\n",
    "        gaussian_process.GaussianProcessClassifier(),\n",
    "        linear_model.LogisticRegressionCV(),\n",
    "        linear_model.PassiveAggressiveClassifier(),\n",
    "        linear_model.RidgeClassifierCV(),\n",
    "        linear_model.SGDClassifier(),\n",
    "        linear_model.Perceptron(),\n",
    "        naive_bayes.BernoulliNB(),\n",
    "        naive_bayes.GaussianNB(),\n",
    "        neighbors.KNeighborsClassifier(),\n",
    "        svm.SVC(probability=True),\n",
    "        tree.DecisionTreeClassifier(),\n",
    "        tree.ExtraTreeClassifier(),\n",
    "        discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "        discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "        XGBClassifier()    \n",
    "        ]\n",
    "\n",
    "    MLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy Mean','MLA Time','MLA RMSE']\n",
    "    MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "    MLA_predict = y_train.copy()\n",
    "\n",
    "    row_index = 0\n",
    "    for alg in MLA:\n",
    "        MLA_name = alg.__class__.__name__\n",
    "        MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "        MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "\n",
    "        start_time = time.time()\n",
    "        alg.fit(X_train, y_train)\n",
    "        predictions = alg.predict(X_test)\n",
    "        alg.fit(df.drop('target', axis=1), df['target'])\n",
    "        last_date_prediction = alg.predict(df.drop('target', axis=1).iloc[-1:])\n",
    "        end_time = time.time()\n",
    "\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "\n",
    "        MLA_compare.loc[row_index, 'MLA Time'] = end_time - start_time\n",
    "        MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = accuracy\n",
    "        MLA_compare.loc[row_index, 'MLA RMSE'] = rmse\n",
    "\n",
    "        MLA_predict[MLA_name] = last_date_prediction\n",
    "\n",
    "        row_index+=1\n",
    "\n",
    "    MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "    return MLA_compare, MLA_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MLA_compare, MLA_predict = run_ml_models(X_train, y_train, X_test, y_test,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 00:00:00 2022-08-01 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/VerTebr0/opt/anaconda3/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/VerTebr0/opt/anaconda3/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bn/74x_l6t57mncrtd65lq_drbw0000gn/T/ipykernel_30323/2311635595.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m     }, ignore_index=True)\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Passer au jour suivant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mcurrent_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;31m# Afficher les prédictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Supposons que df_full est votre DataFrame complet qui contient toutes les données\n",
    "\n",
    "# Fonction pour créer un DataFrame pour une période donnée\n",
    "def create_df(start_date, end_date):\n",
    "    # Pour cet exemple, nous allons simplement filtrer df_full pour obtenir les données dans la période spécifiée.\n",
    "    df = df_btc[(df_btc['date'] >= start_date) & (df_btc['date'] < end_date)]\n",
    "    return df\n",
    "\n",
    "def predict_next_day(X_train, y_train, X_test, y_test, df):\n",
    "    MLA_compare, MLA_predict = run_ml_models(X_train, y_train, X_test, y_test, df)\n",
    "    \n",
    "    # Calculer le ratio Accuracy/RMSE pour chaque modèle\n",
    "    MLA_compare['Accuracy/RMSE'] = MLA_compare['MLA Test Accuracy Mean'] / MLA_compare['MLA RMSE']\n",
    "    \n",
    "    # Trouver le modèle avec le meilleur ratio Accuracy/RMSE\n",
    "    MLA_compare['Accuracy/RMSE'] = pd.to_numeric(MLA_compare['Accuracy/RMSE'], errors='coerce')\n",
    "    best_model = MLA_compare.loc[MLA_compare['Accuracy/RMSE'].idxmax()]\n",
    "    print(best_model['MLA Name'])\n",
    "    \n",
    "    # Retourner les prédictions du meilleur modèle, le nom du modèle, l'accuracy et le RMSE\n",
    "    return MLA_predict[best_model['MLA Name']], best_model['MLA Name'], best_model['MLA Test Accuracy Mean'], best_model['MLA RMSE']\n",
    "\n",
    "# Initialiser la date de début et la date de fin\n",
    "prediction_start_date = datetime(2022, 8, 1)\n",
    "# end_date = df_btc['date'].max()\n",
    "end_date = datetime(2022, 8, 31)\n",
    "\n",
    "\n",
    "# Initialiser le DataFrame pour stocker les prédictions\n",
    "predictions_df = pd.DataFrame(columns=['date', 'prediction'])\n",
    "\n",
    "# Boucle sur chaque jour entre la date de début et la date de fin\n",
    "current_date = prediction_start_date\n",
    "while current_date <= end_date:\n",
    "    # Définir la date de début de la fenêtre glissante\n",
    "    current_date += timedelta(days=1)\n",
    "    window_start_date = current_date - timedelta(days=6*20)  # Approximativement 6 mois\n",
    "    # Créer le DataFrame pour la période de la fenêtre glissante\n",
    "\n",
    "    df = create_df(window_start_date, current_date)\n",
    "    print(df['date'].min(), df['date'].max())\n",
    "\n",
    "    # Supposons que df est votre DataFrame\n",
    "    train_ratio = 0.8\n",
    "    train_size = int(len(df) * train_ratio)\n",
    "\n",
    "    df_train = df.iloc[:train_size]\n",
    "    df_test = df.iloc[train_size:]\n",
    "\n",
    "    df_train = df_train.set_index('date')\n",
    "    df_test = df_test.set_index('date')\n",
    "\n",
    "    X_train = df_train.drop('target',axis=1)\n",
    "    y_train = df_train['target']\n",
    "    X_test = df_test.drop('target',axis=1)\n",
    "    y_test = df_test['target']\n",
    "\n",
    "    #drop la colonne date\n",
    "    df = df.set_index('date')\n",
    "    \n",
    "    # Prédire la valeur pour le jour suivant\n",
    "    next_day_prediction, best_model_name, best_model_accuracy, best_model_rmse = predict_next_day(X_train, y_train, X_test, y_test, df)\n",
    "    \n",
    "    # Ajouter la prédiction, le nom du modèle, l'accuracy et le RMSE au DataFrame des prédictions\n",
    "    predictions_df = predictions_df.append({\n",
    "        'date': current_date, \n",
    "        'prediction': next_day_prediction,\n",
    "        'model': best_model_name,\n",
    "        'accuracy': best_model_accuracy,\n",
    "        'RMSE': best_model_rmse\n",
    "    }, ignore_index=True)\n",
    "    # Passer au jour suivant\n",
    "    current_date = df['date'].shift(-1).dropna().iloc[0]\n",
    "# Afficher les prédictions\n",
    "print(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionner predictions_df et df_btc sur la colonne 'date'\n",
    "df_merged = pd.merge(predictions_df, df_btc[['date', 'target']], on='date')\n",
    "\n",
    "# Comparer les colonnes 'prediction' et 'target'\n",
    "df_merged['correct'] = df_merged['prediction'] == df_merged['target']\n",
    "\n",
    "# Compter le nombre de prédictions correctes\n",
    "num_correct_predictions = df_merged['correct'].sum()\n",
    "\n",
    "print(f\"Nombre de prédictions correctes : {num_correct_predictions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
